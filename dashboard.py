# -*- coding: utf-8 -*-
"""Dashboard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lppZCPF3WBCPThg2ECwX8FH3dL0aojap
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from IPython.display import display


class InventoryEnv:
    """
    Simulates an inventory management environment for a single product.
    The environment handles stock levels, daily demand, and calculates rewards
    based on restocking actions and demand fulfillment.
    """
    def __init__(self, initial_stock=50, max_stock=200, daily_demand_range=(5, 20), product_name="Power Bank"):
        """
        Initializes the inventory environment.

        Args:
            initial_stock (int): The starting stock level for the product.
            max_stock (int): The maximum capacity of the inventory.
            daily_demand_range (tuple): A tuple (min_demand, max_demand) defining the
                                        range from which daily demand is randomly drawn.
            product_name (str): The name of the product being managed.
        """
        self.product_name = product_name
        self.initial_stock = initial_stock
        self.max_stock = max_stock
        self.daily_demand_range = daily_demand_range
        self.actions = [0, 10, 20, 50]
        self.current_stock = initial_stock
        self.time_step = 0
        self.history = []


        self.stock_bins = [0, 10, 20, 50, 100, 150, max_stock + 1]
        self.num_stock_states = len(self.stock_bins) - 1

    def _get_state(self):
        """
        Maps the current continuous stock level to a discrete state index.
        For example, if stock_bins are [0, 10, 20, 50, ...], then:
        - stock < 10 -> state 0
        - 10 <= stock < 20 -> state 1
        - etc.
        """
        for i, upper_bound in enumerate(self.stock_bins[1:]):
            if self.current_stock < upper_bound:
                return i

        return self.num_stock_states - 1

    def reset(self):
        """
        Resets the environment to its initial state for a new episode (e.g., a new year of simulation).
        Clears history and resets stock and time step.
        """
        self.current_stock = self.initial_stock
        self.time_step = 0
        self.history = []
        return self._get_state()

    def step(self, action_index):
        """
        Executes one time step in the environment based on the agent's chosen action.

        Args:
            action_index (int): The index of the action chosen by the agent (corresponds to self.actions).

        Returns:
            tuple: (next_state, reward, done, info)
                - next_state (int): The new discrete state after the action and demand.
                - reward (float): The immediate reward received for the action.
                - done (bool): True if the episode has ended, False otherwise.
                - info (dict): A dictionary for additional debugging or logging information.
        """
        action_units = self.actions[action_index]
        reward = 0
        done = False
        info = {}


        self.current_stock += action_units

        if self.current_stock > self.max_stock:
            self.current_stock = self.max_stock


        daily_demand = np.random.randint(self.daily_demand_range[0], self.daily_demand_range[1] + 1)


        if self.current_stock >= daily_demand:

            reward += 10
            self.current_stock -= daily_demand
            stockout_occurred = False
        else:

            reward -= 10
            stockout_occurred = True
            self.current_stock = 0


        if self.current_stock > (self.max_stock * 0.7):
            reward -= 5


        self.history.append({
            "time_step": self.time_step,
            "stock_before_action": self.current_stock - daily_demand + action_units,
            "action": action_units,
            "stock_after_action_before_demand": self.current_stock + daily_demand,
            "demand": daily_demand,
            "stock_after_demand": self.current_stock,
            "reward": reward,
            "stockout_occurred": stockout_occurred
        })

        self.time_step += 1 # Advance time step.
        next_state = self._get_state() # Determine the new discrete state.


        if self.time_step >= 365:
            done = True

        return next_state, reward, done, info

    def get_action_space_size(self):
        """Returns the number of possible actions."""
        return len(self.actions)

    def get_state_space_size(self):
        """Returns the number of discrete states."""
        return self.num_stock_states


class QLearningAgent:
    """
    Implements a Q-Learning agent to learn an optimal policy for inventory management
    within a simulated environment. The agent learns to choose restocking actions
    based on the current inventory state to maximize cumulative rewards.
    """
    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay_rate=0.001, min_epsilon=0.01):
        """
        Initializes the Q-Learning agent with its environment and hyperparameters.

        Args:
            env (InventoryEnv): The custom inventory environment instance the agent will interact with.
                                This environment defines the state space, action space, and reward system.
            learning_rate (float): Alpha (Î±). Determines how much new information (or the new value)
                                   overrides the old information. A value of 0 means the agent learns nothing,
                                   while a value of 1 means the agent only considers the most recent information.
                                   Typically set between 0.01 and 0.5. A higher learning rate leads to faster
                                   but potentially less stable learning.
            discount_factor (float): Gamma (Î³). Determines the importance of future rewards. A value of 0
                                     makes the agent "myopic" (only considers immediate rewards), while a
                                     value closer to 1 makes it consider long-term rewards. This is crucial
                                     for tasks where actions have delayed consequences, like inventory management.
                                     Commonly set between 0.9 and 0.999.
            epsilon (float): Epsilon (Îµ). The initial probability of choosing a random action (exploration)
                             instead of the best-known action (exploitation). This helps the agent discover
                             new, potentially better, strategies. Starts high to encourage initial exploration.
            epsilon_decay_rate (float): The rate at which epsilon decreases over episodes. As the agent learns
                                        more, it should explore less and exploit its knowledge more. A smaller
                                        decay rate means exploration continues for longer.
            min_epsilon (float): The minimum value epsilon can decay to. Ensures the agent always maintains
                                 a small degree of exploration to avoid getting stuck in local optima,
                                 especially in dynamic environments.
        """
        self.env = env
        self.alpha = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay_rate = epsilon_decay_rate
        self.min_epsilon = min_epsilon

        self.q_table = np.zeros((env.get_state_space_size(), env.get_action_space_size()))

    def choose_action(self, state):
        """
        Selects an action based on the epsilon-greedy policy.
        With probability epsilon, a random action is chosen (exploration).
        Otherwise, the action with the highest Q-value for the current state is chosen (exploitation).

        Args:
            state (int): The current discrete state of the environment.

        Returns:
            int: The index of the chosen action.
        """
        if np.random.uniform(0, 1) < self.epsilon:
            return np.random.randint(self.env.get_action_space_size()) # Explore: choose a random action
        else:
            return np.argmax(self.q_table[state, :]) # Exploit: choose the action with the highest Q-value

    def update_q_table(self, state, action, reward, next_state):
        """
        Updates the Q-value for a given state-action pair using the Q-Learning update rule.

        Args:
            state (int): The state before the action was taken.
            action (int): The action taken.
            reward (float): The immediate reward received after taking the action.
            next_state (int): The state after the action was taken and the environment transitioned.
        """
        old_value = self.q_table[state, action]
        next_max = np.max(self.q_table[next_state, :])


        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)
        self.q_table[state, action] = new_value

    def decay_epsilon(self):
        """
        Decreases the epsilon value over time, gradually reducing the rate of exploration.
        Epsilon will not drop below min_epsilon.
        """
        self.epsilon = max(self.min_epsilon, self.epsilon - self.epsilon_decay_rate)

    def train(self, num_episodes=1000):
        """
        Trains the Q-learning agent for a specified number of episodes.
        Each episode represents a full simulation run (e.g., one year of inventory management).

        Args:
            num_episodes (int): The total number of training episodes.

        Returns:
            tuple: (rewards_per_episode, q_table)
                - rewards_per_episode (list): A list containing the total reward accumulated
                                              in each training episode.
                - q_table (numpy.ndarray): The final learned Q-table after training.
        """
        rewards_per_episode = []

        print(f"Starting Q-Learning training for {num_episodes} episodes...")
        print(f"Initial Epsilon: {self.epsilon:.4f}, Learning Rate: {self.alpha}, Discount Factor: {self.gamma}")

        for episode in range(num_episodes):
            state = self.env.reset()
            done = False
            episode_reward = 0


            while not done:
                action = self.choose_action(state)
                next_state, reward, done, _ = self.env.step(action)
                self.update_q_table(state, action, reward, next_state)

                state = next_state
                episode_reward += reward

            self.decay_epsilon()
            rewards_per_episode.append(episode_reward)


            if (episode + 1) % 100 == 0:
                print(f"Episode {episode + 1}/{num_episodes}, Current Epsilon: {self.epsilon:.4f}, Total Reward: {episode_reward}")

        print("Training complete!")
        return rewards_per_episode, self.q_table

    def evaluate_policy(self, num_episodes=100):
        """
        Evaluates the performance of the learned policy (Q-table) over multiple episodes.
        During evaluation, the agent always chooses the greedy action (exploitation only).

        Args:
            num_episodes (int): The number of episodes to run for evaluation.

        Returns:
            tuple: (mean_reward, std_reward)
                - mean_reward (float): The average total reward obtained during evaluation episodes.
                - std_reward (float): The standard deviation of total rewards during evaluation.
        """
        total_rewards = []
        print(f"\nEvaluating learned policy over {num_episodes} episodes...")
        for i in range(num_episodes):
            state = self.env.reset()
            done = False
            episode_reward = 0
            while not done:

                action = np.argmax(self.q_table[state, :])
                next_state, reward, done, _ = self.env.step(action)
                state = next_state
                episode_reward += reward
            total_rewards.append(episode_reward)
            if (i + 1) % 20 == 0:
                print(f"  Evaluation Episode {i+1}/{num_episodes}, Reward: {episode_reward}")

        mean_reward = np.mean(total_rewards)
        std_reward = np.std(total_rewards)
        return mean_reward, std_reward




env = InventoryEnv(initial_stock=50, max_stock=200, daily_demand_range=(5, 20))

agent = QLearningAgent(env, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay_rate=0.001, min_epsilon=0.01)


rewards, q_table = agent.train(num_episodes=2000)


plt.figure(figsize=(12, 6))
plt.plot(rewards)
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("Q-Learning Agent: Reward per Episode (Learning Curve)")
plt.grid(True)
plt.show()


mean_reward, std_reward = agent.evaluate_policy(num_episodes=100)
print(f"\nEvaluation over 100 episodes: Mean Reward = {mean_reward:.2f}, Std Dev = {std_reward:.2f}")


print("\nLearned Q-Table:")

q_table_df = pd.DataFrame(q_table, columns=[f"Order {a} units" for a in env.actions])
q_table_df.index = [f"Stock State {i}" for i in range(env.get_state_space_size())]
display(q_table_df)



print("\n--- Running a single simulation with learned policy ---")
env_sim = InventoryEnv(initial_stock=50, max_stock=200, daily_demand_range=(5, 20))
state = env_sim.reset()
done = False
sim_reward = 0
sim_steps = 0

while not done:
    action = np.argmax(q_table[state, :])
    next_state, reward, done, _ = env_sim.step(action)
    state = next_state
    sim_reward += reward
    sim_steps += 1

import altair as alt

melted_df = q_table_df.melt(value_vars=q_table_df.columns, var_name='Order Type', value_name='Value')

chart = alt.Chart(melted_df).mark_arc().encode(
    theta=alt.Theta("Value", stack=True),
    color=alt.Color("Order Type"),
    order=alt.Order("Value", sort="descending")
)
chart

from matplotlib import pyplot as plt
q_table_df['Order 10 units'].plot(kind='hist', bins=20, title='Order 10 units')
plt.gca().spines[['top', 'right',]].set_visible(False)

!pip install streamlit pyngrok openpyxl langchain openai

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta


from ipywidgets import interact, IntSlider, Dropdown, Text, Button, Output, VBox, HBox
from IPython.display import display, clear_output


try:
    df_sales = pd.read_csv("Cleaned_Sales_Inventory_For_PowerBI.xlsx - Sales.csv")
    df_inventory = pd.read_csv("Cleaned_Sales_Inventory_For_PowerBI.xlsx - Inventory.csv")


    df_sales["Date"] = pd.to_datetime(df_sales["Date"])
    df_inventory["Date"] = pd.to_datetime(df_inventory["Date"])

    print("Data loaded successfully!")
    print("\nSales Data Head:")
    display(df_sales.head())
    print("\nInventory Data Head:")
    display(df_inventory.head())

except FileNotFoundError:
    print("Error: Make sure 'Cleaned_Sales_Inventory_For_PowerBI.xlsx - Sales.csv' and 'Cleaned_Sales_Inventory_For_PowerBI.xlsx - Inventory.csv' are in the same directory as this notebook.")
    print("Generating dummy data for demonstration purposes...")

    def generate_dummy_data(num_products=10, num_days=180, avg_sales_per_day=50):
        """Generates dummy sales and inventory data."""
        products = [f"Product_{i+1}" for i in range(num_products)]
        product_categories = {
            f"Product_{i+1}": np.random.choice(["Power Bank", "Headset", "Cable", "Charger"])
            for i in range(num_products)
        }


        sales_data = []
        start_date = datetime.now() - timedelta(days=num_days)
        for i in range(num_days):
            current_date = start_date + timedelta(days=i)
            num_transactions = np.random.randint(20, avg_sales_per_day * 2)
            for _ in range(num_transactions):
                product = np.random.choice(products)
                quantity = np.random.randint(1, 5)
                price = np.random.uniform(10, 100)
                sales_data.append({
                    "Date": current_date.strftime("%Y-%m-%d"),
                    "Product": product,
                    "Category": product_categories[product],
                    "Quantity": quantity,
                    "Price": round(price, 2),
                    "Total Sales": round(quantity * price, 2)
                })
        df_sales = pd.DataFrame(sales_data)
        df_sales["Date"] = pd.to_datetime(df_sales["Date"])


        inventory_data = []
        initial_stock = {p: np.random.randint(50, 200) for p in products}
        current_stock = initial_stock.copy()

        for i in range(num_days):
            current_date = start_date + timedelta(days=i)
            daily_sales_summary = df_sales[df_sales['Date'] == current_date]['Product'].value_counts()
            for product, sales_qty in daily_sales_summary.items():
                current_stock[product] -= sales_qty
                if current_stock[product] < 0:
                    current_stock[product] = 0

            for product in products:
                if current_stock[product] < 20 and np.random.rand() < 0.3:
                    restock_qty = np.random.choice([10, 20, 50])
                    current_stock[product] += restock_qty

            for product in products:
                inventory_data.append({
                    "Date": current_date.strftime("%Y-%m-%d"),
                    "Product": product,
                    "Category": product_categories[product],
                    "Current Stock": current_stock[product]
                })
        df_inventory = pd.DataFrame(inventory_data)
        df_inventory["Date"] = pd.to_datetime(df_inventory["Date"])
        return df_sales, df_inventory

    df_sales, df_inventory = generate_dummy_data()
    print("Dummy data generated and loaded.")
    print("\nSales Data Head (Dummy):")
    display(df_sales.head())
    print("\nInventory Data Head (Dummy):")
    display(df_inventory.head())




print("--- Sales & Inventory Overview Dashboard ---")


total_sales_value = df_sales["Total Sales"].sum()
total_products_sold = df_sales["Quantity"].sum()
unique_products = df_inventory["Product"].nunique()

print(f"Total Revenue: ${total_sales_value:,.2f}")
print(f"Total Units Sold: {total_products_sold:,.0f}")
print(f"Total Products: {unique_products:,.0f}")
print("\n" + "="*50 + "\n")



print("--- Top-Selling Products ---")
def plot_top_selling_products(top_n):
    top_selling_products = df_sales.groupby("Product")["Quantity"].sum().sort_values(ascending=False).head(top_n)
    plt.figure(figsize=(10, 5))
    sns.barplot(x=top_selling_products.values, y=top_selling_products.index, palette="viridis")
    plt.xlabel("Quantity Sold")
    plt.ylabel("Product")
    plt.title(f"Top {top_n} Selling Products by Quantity")
    plt.tight_layout()
    plt.show()

interact(plot_top_selling_products, top_n=IntSlider(min=3, max=10, step=1, value=5, description='Show Top N:'));

print("\n" + "="*50 + "\n")



print("--- Low-Stock Alerts ---")
def display_low_stock_alerts(low_stock_threshold):
    current_inventory = df_inventory.drop_duplicates(subset=['Product'], keep='last')
    low_stock_items = current_inventory[current_inventory["Current Stock"] <= low_stock_threshold].sort_values(by="Current Stock")

    if not low_stock_items.empty:
        print(f"ðŸš¨ Attention! The following items are running low on stock (below {low_stock_threshold} units):")
        display(low_stock_items[["Product", "Category", "Current Stock"]].reset_index(drop=True))
    else:
        print("ðŸŽ‰ All products appear to be well-stocked!")

interact(display_low_stock_alerts, low_stock_threshold=IntSlider(min=5, max=50, step=5, value=20, description='Threshold:'));

print("\n" + "="*50 + "\n")



print("--- Sales Trend Over Time ---")
def plot_sales_trend(time_granularity):
    if time_granularity == "Daily":
        sales_trend = df_sales.groupby("Date")["Total Sales"].sum().sort_index()
    elif time_granularity == "Weekly":
        sales_trend = df_sales.set_index("Date").resample("W")["Total Sales"].sum()
    else:
        sales_trend = df_sales.set_index("Date").resample("M")["Total Sales"].sum()

    plt.figure(figsize=(12, 6))
    plt.plot(sales_trend.index, sales_trend.values, marker='o', linestyle='-')
    plt.xlabel("Date")
    plt.ylabel("Total Sales ($)")
    plt.title(f"{time_granularity} Sales Trend")
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

interact(plot_sales_trend, time_granularity=Dropdown(options=["Daily", "Weekly", "Monthly"], value="Daily", description='Granularity:'));

print("\n" + "="*50 + "\n")



print("--- Smart Sales Assistant Chatbot ---")
print("This is a placeholder for the chatbot. In a full implementation, this would integrate with LangChain or a similar LLM framework.")
print("Type your query below and press Enter or click 'Send'.")


chat_history = []


chat_output = Output()


user_input = Text(
    value='',
    placeholder='Type your query here (e.g., "top-selling products", "low stock items")',
    description='You:',
    disabled=False
)

send_button = Button(description="Send")

def get_chatbot_response(query):
    """
    Placeholder function for chatbot response logic.
    In a real scenario, this would call a LangChain agent.
    """
    query_lower = query.lower()
    if "top-selling" in query_lower or "best-selling" in query_lower:
        return "I can tell you about top-selling products. Please specify a time period if you like!"
    elif "low stock" in query_lower or "restock" in query_lower:
        return "I can provide low stock alerts and insights for restocking. What product are you interested in?"
    elif "sales" in query_lower and ("month" in query_lower or "week" in query_lower or "day" in query_lower):
        return "I can tell you about daily, weekly, or monthly sales trends."
    elif "hello" in query_lower or "hi" in query_lower:
        return "Hello! How can I assist you with sales or inventory data today?"
    else:
        return "I'm sorry, I can only answer questions related to sales, inventory, and restocking based on the available data. Please try asking about top-selling items, low stock, or sales trends."

def on_send_button_clicked(b):
    with chat_output:
        clear_output(wait=True) # Clear previous output to update chat history
        query = user_input.value
        if query:
            chat_history.append({"role": "user", "content": query})
            response = get_chatbot_response(query)
            chat_history.append({"role": "assistant", "content": response})
            user_input.value = '' # Clear input field

        # Display all messages in history
        for msg in chat_history:
            if msg["role"] == "user":
                print(f"You: {msg['content']}")
            else:
                print(f"Assistant: {msg['content']}")


send_button.on_click(on_send_button_clicked)


display(VBox([chat_output, HBox([user_input, send_button])]))

print("\n" + "="*50 + "\n")